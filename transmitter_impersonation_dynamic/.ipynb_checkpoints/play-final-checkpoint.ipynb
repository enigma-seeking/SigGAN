{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WirelessSystem.system import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from SigGAN.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(sig_u):\n",
    "    pwr = np.sqrt(np.mean(np.sum(sig_u**2,axis = -1),axis = -1))\n",
    "    sig_u = sig_u/pwr[:,None,None]\n",
    "    print(sig_u.shape)\n",
    "    return sig_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(vec1,vec2,seed = 0):\n",
    "    np.random.seed(0)\n",
    "    shfl_indx = np.arange(vec1.shape[0])\n",
    "    np.random.shuffle(shfl_indx)\n",
    "    shfl_indx = shfl_indx.astype('int')\n",
    "    vec1 = vec1[shfl_indx]\n",
    "    vec2 = np.copy(vec2[shfl_indx])\n",
    "    return vec1,vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(vec,n_train,n_valid,n_test):\n",
    "    vec_train = vec[0:n_train]\n",
    "    vec_valid = vec[n_train:n_train+n_valid]\n",
    "    vec_test = vec[n_train+n_valid:]\n",
    "    return vec_train,vec_valid,vec_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols = 256 // 2\n",
    "n_authorized = 5\n",
    "n_unauthorized = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_system = RFSystem(n_authorized = n_authorized, n_unauthorized = n_unauthorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_auth_, txid_auth = rf_system.get_n_received_symbol_blocks(1000, n_symbols, authorized = 0)\n",
    "sig_unauth_, txid_unauth = rf_system.get_n_received_symbol_blocks(1000, n_symbols, authorized = 1)\n",
    "sig_impersonate_, _ = rf_system.get_n_received_symbol_blocks(10, n_symbols, authorized = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_auth=sig_auth_\n",
    "sig_unauth=sig_unauth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_impersonate_ad_, _ = rf_system.get_n_received_symbol_blocks(1000, n_symbols, authorized = 2)\n",
    "sig_impersonate_ad=sig_impersonate_ad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_impersonate=sig_impersonate_.reshape((-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_impersonate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 256, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sig_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "B = 64 # batch size\n",
    "T = 256 # Max length of sentence\n",
    "g_H = 100 # Generator LSTM hidden size\n",
    "g_lr = 1e-3\n",
    "d_dropout = 0.5 # dropout ratio\n",
    "d_lr = 1e-3\n",
    "\n",
    "n_sample=1 # Number of Monte Calro Search\n",
    "generate_samples = 2000 # Number of generated sentences\n",
    "\n",
    "# Pretraining parameters\n",
    "g_pre_lr = 1e-3\n",
    "d_pre_lr = 1e-3\n",
    "g_pre_epochs= 60\n",
    "d_pre_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(rf_system, B, T, n_authorized, g_H, d_dropout, g_lr=g_lr, d_lr=d_lr, n_sample=n_sample, generate_samples=generate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rd = np.concatenate([sig_auth,sig_impersonate_ad])\n",
    "txid_rd = np.concatenate([txid_auth,np.ones((sig_impersonate_ad.shape[0],))*n_authorized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rd, txid_rd = shuffle(sig_rd, txid_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "txid_disc = txid_rd == n_authorized\n",
    "txid_disc = np.invert(txid_disc)\n",
    "txid_disc = txid_disc.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0. 1. 2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(txid_disc))\n",
    "print(np.unique(txid_rd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.1\n",
    "valid_frac  = 0.2\n",
    "\n",
    "n_samples  = sig_rd.shape[0]\n",
    "\n",
    "n_test = int(test_frac*n_samples)\n",
    "n_valid = int(valid_frac*n_samples)\n",
    "n_train = n_samples - n_test - n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rd_train,sig_rd_valid,sig_rd_test=split(sig_rd,n_train,n_valid,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train,out_valid,out_test=split(txid_disc,n_train,n_valid,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 2)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 256, 2, 1)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1_conv1 (Conv2D)                (None, 256, 2, 16)   32          reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "1_conv2 (Conv2D)                (None, 256, 2, 16)   1552        1_conv1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1_bn (BatchNormalization)       (None, 256, 2, 16)   64          1_conv2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1_act1 (Activation)             (None, 256, 2, 16)   0           1_bn[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "1_conv3 (Conv2D)                (None, 256, 2, 16)   1552        1_act1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "1_bn2 (BatchNormalization)      (None, 256, 2, 16)   64          1_conv3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1_add (Add)                     (None, 256, 2, 16)   0           1_bn2[0][0]                      \n",
      "                                                                 1_conv1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1_act2 (Activation)             (None, 256, 2, 16)   0           1_add[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "3_0_conv1 (Conv2D)              (None, 256, 2, 32)   544         1_act2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "3_0_conv2 (Conv2D)              (None, 256, 2, 32)   6176        3_0_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "3_0_bn (BatchNormalization)     (None, 256, 2, 32)   128         3_0_conv2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "3_0_act1 (Activation)           (None, 256, 2, 32)   0           3_0_bn[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "3_0_conv3 (Conv2D)              (None, 256, 2, 32)   6176        3_0_act1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3_0_bn2 (BatchNormalization)    (None, 256, 2, 32)   128         3_0_conv3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "3_0_add (Add)                   (None, 256, 2, 32)   0           3_0_bn2[0][0]                    \n",
      "                                                                 3_0_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "3_0_act2 (Activation)           (None, 256, 2, 32)   0           3_0_add[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 2, 32)   0           3_0_act2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 2, 16)   528         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           40970       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sftmx_0 (Dense)                 (None, 1)            11          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 57,925\n",
      "Trainable params: 57,733\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator pre-training\n",
      "Train on 1400 samples, validate on 400 samples\n",
      "Epoch 1/200\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 1.0623 - val_loss: 0.9126\n",
      "Epoch 2/200\n",
      "1400/1400 [==============================] - 0s 186us/step - loss: 0.9233 - val_loss: 0.9093\n",
      "Epoch 3/200\n",
      "1400/1400 [==============================] - 0s 175us/step - loss: 0.9153 - val_loss: 0.9093\n",
      "Epoch 4/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.9124 - val_loss: 0.9076\n",
      "Epoch 5/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.9117 - val_loss: 0.9072\n",
      "Epoch 6/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.9075 - val_loss: 0.9061\n",
      "Epoch 7/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.9001 - val_loss: 0.9034\n",
      "Epoch 8/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.9019 - val_loss: 0.9003\n",
      "Epoch 9/200\n",
      "1400/1400 [==============================] - 0s 195us/step - loss: 0.8967 - val_loss: 0.8993\n",
      "Epoch 10/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.8892 - val_loss: 0.8978\n",
      "Epoch 11/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.8905 - val_loss: 0.8971\n",
      "Epoch 12/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.8871 - val_loss: 0.8955\n",
      "Epoch 13/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.8815 - val_loss: 0.8955\n",
      "Epoch 14/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.8737 - val_loss: 0.8931\n",
      "Epoch 15/200\n",
      "1400/1400 [==============================] - 0s 169us/step - loss: 0.8719 - val_loss: 0.8924\n",
      "Epoch 16/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.8625 - val_loss: 0.8891\n",
      "Epoch 17/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.8545 - val_loss: 0.8894\n",
      "Epoch 18/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.8536 - val_loss: 0.8859\n",
      "Epoch 19/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.8473 - val_loss: 0.8862\n",
      "Epoch 20/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.8393 - val_loss: 0.8810\n",
      "Epoch 21/200\n",
      "1400/1400 [==============================] - 0s 155us/step - loss: 0.8322 - val_loss: 0.8782\n",
      "Epoch 22/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.8216 - val_loss: 0.8726\n",
      "Epoch 23/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.8136 - val_loss: 0.8763\n",
      "Epoch 24/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.7980 - val_loss: 0.8650\n",
      "Epoch 25/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.7931 - val_loss: 0.8659\n",
      "Epoch 26/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.7815 - val_loss: 0.8573\n",
      "Epoch 27/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.7659 - val_loss: 0.8519\n",
      "Epoch 28/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.7552 - val_loss: 0.8473\n",
      "Epoch 29/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.7459 - val_loss: 0.8280\n",
      "Epoch 30/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.7371 - val_loss: 0.8545\n",
      "Epoch 31/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.7071 - val_loss: 0.8812\n",
      "Epoch 32/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.6962 - val_loss: 0.8170\n",
      "Epoch 33/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.6951 - val_loss: 0.8413\n",
      "Epoch 34/200\n",
      "1400/1400 [==============================] - 0s 191us/step - loss: 0.6847 - val_loss: 0.8486\n",
      "Epoch 35/200\n",
      "1400/1400 [==============================] - 0s 167us/step - loss: 0.6722 - val_loss: 0.8690\n",
      "Epoch 36/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.6274 - val_loss: 0.9139\n",
      "Epoch 37/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.6238 - val_loss: 0.9972\n",
      "Epoch 38/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.6096 - val_loss: 0.9121\n",
      "Epoch 39/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.5953 - val_loss: 0.9050\n",
      "Epoch 40/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.5939 - val_loss: 0.8935\n",
      "Epoch 41/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.5888 - val_loss: 0.8214\n",
      "Epoch 42/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.5759 - val_loss: 0.8873\n",
      "Epoch 43/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.5756 - val_loss: 0.7965\n",
      "Epoch 44/200\n",
      "1400/1400 [==============================] - 0s 170us/step - loss: 0.5677 - val_loss: 0.7431\n",
      "Epoch 45/200\n",
      "1400/1400 [==============================] - 0s 155us/step - loss: 0.5576 - val_loss: 0.6660\n",
      "Epoch 46/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.5725 - val_loss: 0.7151\n",
      "Epoch 47/200\n",
      "1400/1400 [==============================] - 0s 170us/step - loss: 0.5363 - val_loss: 0.7757\n",
      "Epoch 48/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.5459 - val_loss: 0.6505\n",
      "Epoch 49/200\n",
      "1400/1400 [==============================] - 0s 167us/step - loss: 0.5378 - val_loss: 0.8069\n",
      "Epoch 50/200\n",
      "1400/1400 [==============================] - 0s 167us/step - loss: 0.5179 - val_loss: 0.8276\n",
      "Epoch 51/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.5134 - val_loss: 0.7108\n",
      "Epoch 52/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.5231 - val_loss: 0.6801\n",
      "Epoch 53/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.5109 - val_loss: 0.5910\n",
      "Epoch 54/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.5047 - val_loss: 0.5890\n",
      "Epoch 55/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.5059 - val_loss: 0.6541\n",
      "Epoch 56/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.5141 - val_loss: 0.5929\n",
      "Epoch 57/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.4957 - val_loss: 0.6917\n",
      "Epoch 58/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.5090 - val_loss: 0.5985\n",
      "Epoch 59/200\n",
      "1400/1400 [==============================] - 0s 156us/step - loss: 0.4730 - val_loss: 0.6074\n",
      "Epoch 60/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.4900 - val_loss: 0.5939\n",
      "Epoch 61/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.4714 - val_loss: 0.6192\n",
      "Epoch 62/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.4818 - val_loss: 0.5836\n",
      "Epoch 63/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.4804 - val_loss: 0.5818\n",
      "Epoch 64/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.4714 - val_loss: 0.5803\n",
      "Epoch 65/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.4463 - val_loss: 0.5812\n",
      "Epoch 66/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.4543 - val_loss: 0.6066\n",
      "Epoch 67/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.4682 - val_loss: 0.5779\n",
      "Epoch 68/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.4541 - val_loss: 0.5732\n",
      "Epoch 69/200\n",
      "1400/1400 [==============================] - 0s 168us/step - loss: 0.4489 - val_loss: 0.6083\n",
      "Epoch 70/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.4546 - val_loss: 0.5768\n",
      "Epoch 71/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.4566 - val_loss: 0.5763\n",
      "Epoch 72/200\n",
      "1400/1400 [==============================] - 0s 155us/step - loss: 0.4452 - val_loss: 0.5764\n",
      "Epoch 73/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.4433 - val_loss: 0.5737\n",
      "Epoch 74/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.4207 - val_loss: 0.5976\n",
      "Epoch 75/200\n",
      "1400/1400 [==============================] - 0s 163us/step - loss: 0.4342 - val_loss: 0.5797\n",
      "Epoch 76/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.4428 - val_loss: 0.6138\n",
      "Epoch 77/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.4311 - val_loss: 0.6031\n",
      "Epoch 78/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.4132 - val_loss: 0.5808\n",
      "Epoch 79/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.4307 - val_loss: 0.5734\n",
      "Epoch 80/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.4321 - val_loss: 0.6099\n",
      "Epoch 81/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.4221 - val_loss: 0.5746\n",
      "Epoch 82/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.4233 - val_loss: 0.5933\n",
      "Epoch 83/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.4149 - val_loss: 0.5775\n",
      "Epoch 84/200\n",
      "1400/1400 [==============================] - 0s 192us/step - loss: 0.4158 - val_loss: 0.6647\n",
      "Epoch 85/200\n",
      "1400/1400 [==============================] - 0s 168us/step - loss: 0.4392 - val_loss: 0.5779\n",
      "Epoch 86/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3994 - val_loss: 0.5795\n",
      "Epoch 87/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.4218 - val_loss: 0.5903\n",
      "Epoch 88/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.4062 - val_loss: 0.5944\n",
      "Epoch 89/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.4156 - val_loss: 0.5889\n",
      "Epoch 90/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.4206 - val_loss: 0.6066\n",
      "Epoch 91/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.4028 - val_loss: 0.6389\n",
      "Epoch 92/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.4195 - val_loss: 0.5832\n",
      "Epoch 93/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.4028 - val_loss: 0.5959\n",
      "Epoch 94/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.4104 - val_loss: 0.5915\n",
      "Epoch 95/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3960 - val_loss: 0.5870\n",
      "Epoch 96/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3999 - val_loss: 0.5840\n",
      "Epoch 97/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3909 - val_loss: 0.5948\n",
      "Epoch 98/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3854 - val_loss: 0.5893\n",
      "Epoch 99/200\n",
      "1400/1400 [==============================] - 0s 167us/step - loss: 0.3764 - val_loss: 0.6419\n",
      "Epoch 100/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3966 - val_loss: 0.6008\n",
      "Epoch 101/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3786 - val_loss: 0.6001\n",
      "Epoch 102/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3760 - val_loss: 0.6091\n",
      "Epoch 103/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3822 - val_loss: 0.6128\n",
      "Epoch 104/200\n",
      "1400/1400 [==============================] - 0s 153us/step - loss: 0.3866 - val_loss: 0.5988\n",
      "Epoch 105/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3858 - val_loss: 0.6487\n",
      "Epoch 106/200\n",
      "1400/1400 [==============================] - 0s 167us/step - loss: 0.3758 - val_loss: 0.5975\n",
      "Epoch 107/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3758 - val_loss: 0.6219\n",
      "Epoch 108/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3591 - val_loss: 0.5957\n",
      "Epoch 109/200\n",
      "1400/1400 [==============================] - 0s 163us/step - loss: 0.3945 - val_loss: 0.5896\n",
      "Epoch 110/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.3592 - val_loss: 0.5882\n",
      "Epoch 111/200\n",
      "1400/1400 [==============================] - 0s 168us/step - loss: 0.3709 - val_loss: 0.5965\n",
      "Epoch 112/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.3697 - val_loss: 0.5883\n",
      "Epoch 113/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.3778 - val_loss: 0.6589\n",
      "Epoch 114/200\n",
      "1400/1400 [==============================] - 0s 169us/step - loss: 0.3525 - val_loss: 0.6928\n",
      "Epoch 115/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3671 - val_loss: 0.6073\n",
      "Epoch 116/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3711 - val_loss: 0.6311\n",
      "Epoch 117/200\n",
      "1400/1400 [==============================] - 0s 163us/step - loss: 0.3513 - val_loss: 0.6029\n",
      "Epoch 118/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.3629 - val_loss: 0.5905\n",
      "Epoch 119/200\n",
      "1400/1400 [==============================] - 0s 167us/step - loss: 0.3552 - val_loss: 0.5913\n",
      "Epoch 120/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3663 - val_loss: 0.6143\n",
      "Epoch 121/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.3528 - val_loss: 0.6189\n",
      "Epoch 122/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.3504 - val_loss: 0.6658\n",
      "Epoch 123/200\n",
      "1400/1400 [==============================] - 0s 170us/step - loss: 0.3717 - val_loss: 0.5917\n",
      "Epoch 124/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3811 - val_loss: 0.5901\n",
      "Epoch 125/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3737 - val_loss: 0.6032\n",
      "Epoch 126/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.3486 - val_loss: 0.6031\n",
      "Epoch 127/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3467 - val_loss: 0.6396\n",
      "Epoch 128/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3441 - val_loss: 0.6157\n",
      "Epoch 129/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.3454 - val_loss: 0.6109\n",
      "Epoch 130/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3510 - val_loss: 0.6091\n",
      "Epoch 131/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3713 - val_loss: 0.6568\n",
      "Epoch 132/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.3515 - val_loss: 0.6288\n",
      "Epoch 133/200\n",
      "1400/1400 [==============================] - 0s 156us/step - loss: 0.3413 - val_loss: 0.6921\n",
      "Epoch 134/200\n",
      "1400/1400 [==============================] - 0s 201us/step - loss: 0.3442 - val_loss: 0.6647\n",
      "Epoch 135/200\n",
      "1400/1400 [==============================] - 0s 163us/step - loss: 0.3516 - val_loss: 0.6779\n",
      "Epoch 136/200\n",
      "1400/1400 [==============================] - 0s 165us/step - loss: 0.3475 - val_loss: 0.6056\n",
      "Epoch 137/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3397 - val_loss: 0.7806\n",
      "Epoch 138/200\n",
      "1400/1400 [==============================] - 0s 167us/step - loss: 0.3550 - val_loss: 0.6617\n",
      "Epoch 139/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3441 - val_loss: 0.6102\n",
      "Epoch 140/200\n",
      "1400/1400 [==============================] - 0s 156us/step - loss: 0.3412 - val_loss: 0.6348\n",
      "Epoch 141/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3470 - val_loss: 0.6298\n",
      "Epoch 142/200\n",
      "1400/1400 [==============================] - 0s 163us/step - loss: 0.3470 - val_loss: 0.6333\n",
      "Epoch 143/200\n",
      "1400/1400 [==============================] - 0s 166us/step - loss: 0.3349 - val_loss: 0.6048\n",
      "Epoch 144/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.3630 - val_loss: 0.6094\n",
      "Epoch 145/200\n",
      "1400/1400 [==============================] - 0s 163us/step - loss: 0.3458 - val_loss: 0.6149\n",
      "Epoch 146/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3436 - val_loss: 0.6343\n",
      "Epoch 147/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3434 - val_loss: 0.6526\n",
      "Epoch 148/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3408 - val_loss: 0.6261\n",
      "Epoch 149/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3326 - val_loss: 0.6282\n",
      "Epoch 150/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.3402 - val_loss: 0.6178\n",
      "Epoch 151/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3426 - val_loss: 0.6473\n",
      "Epoch 152/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3375 - val_loss: 0.7294\n",
      "Epoch 153/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3385 - val_loss: 0.6901\n",
      "Epoch 154/200\n",
      "1400/1400 [==============================] - 0s 156us/step - loss: 0.3350 - val_loss: 0.6229\n",
      "Epoch 155/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3273 - val_loss: 0.6264\n",
      "Epoch 156/200\n",
      "1400/1400 [==============================] - 0s 155us/step - loss: 0.3377 - val_loss: 0.6436\n",
      "Epoch 157/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3374 - val_loss: 0.6987\n",
      "Epoch 158/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3465 - val_loss: 0.6206\n",
      "Epoch 159/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3374 - val_loss: 0.6561\n",
      "Epoch 160/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.3047 - val_loss: 0.7082\n",
      "Epoch 161/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3123 - val_loss: 0.6643\n",
      "Epoch 162/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.3424 - val_loss: 0.6943\n",
      "Epoch 163/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3283 - val_loss: 0.7194\n",
      "Epoch 164/200\n",
      "1400/1400 [==============================] - 0s 154us/step - loss: 0.3295 - val_loss: 0.7932\n",
      "Epoch 165/200\n",
      "1400/1400 [==============================] - 0s 156us/step - loss: 0.3118 - val_loss: 0.7071\n",
      "Epoch 166/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3121 - val_loss: 0.7567\n",
      "Epoch 167/200\n",
      "1400/1400 [==============================] - 0s 156us/step - loss: 0.3387 - val_loss: 0.7281\n",
      "Epoch 168/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3215 - val_loss: 0.7532\n",
      "Epoch 169/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3174 - val_loss: 0.6858\n",
      "Epoch 170/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3200 - val_loss: 0.6720\n",
      "Epoch 171/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3370 - val_loss: 0.7346\n",
      "Epoch 172/200\n",
      "1400/1400 [==============================] - 0s 169us/step - loss: 0.3520 - val_loss: 0.6196\n",
      "Epoch 173/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3277 - val_loss: 0.6783\n",
      "Epoch 174/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3012 - val_loss: 0.6539\n",
      "Epoch 175/200\n",
      "1400/1400 [==============================] - 0s 155us/step - loss: 0.3064 - val_loss: 0.7603\n",
      "Epoch 176/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3086 - val_loss: 0.7272\n",
      "Epoch 177/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3151 - val_loss: 0.7235\n",
      "Epoch 178/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3173 - val_loss: 0.6632\n",
      "Epoch 179/200\n",
      "1400/1400 [==============================] - 0s 152us/step - loss: 0.3033 - val_loss: 0.8183\n",
      "Epoch 180/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3163 - val_loss: 0.6574\n",
      "Epoch 181/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3218 - val_loss: 0.9119\n",
      "Epoch 182/200\n",
      "1400/1400 [==============================] - 0s 164us/step - loss: 0.2987 - val_loss: 0.6464\n",
      "Epoch 183/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3207 - val_loss: 0.6980\n",
      "Epoch 184/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3064 - val_loss: 0.6722\n",
      "Epoch 185/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3198 - val_loss: 0.7087\n",
      "Epoch 186/200\n",
      "1400/1400 [==============================] - 0s 162us/step - loss: 0.3183 - val_loss: 0.7321\n",
      "Epoch 187/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3054 - val_loss: 0.6774\n",
      "Epoch 188/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.3173 - val_loss: 0.6482\n",
      "Epoch 189/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3135 - val_loss: 0.6575\n",
      "Epoch 190/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3050 - val_loss: 0.6943\n",
      "Epoch 191/200\n",
      "1400/1400 [==============================] - 0s 170us/step - loss: 0.3190 - val_loss: 0.8415\n",
      "Epoch 192/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3109 - val_loss: 0.6434\n",
      "Epoch 193/200\n",
      "1400/1400 [==============================] - 0s 155us/step - loss: 0.3074 - val_loss: 0.6813\n",
      "Epoch 194/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3069 - val_loss: 0.6750\n",
      "Epoch 195/200\n",
      "1400/1400 [==============================] - 0s 160us/step - loss: 0.3024 - val_loss: 0.7072\n",
      "Epoch 196/200\n",
      "1400/1400 [==============================] - 0s 158us/step - loss: 0.3032 - val_loss: 0.6713\n",
      "Epoch 197/200\n",
      "1400/1400 [==============================] - 0s 161us/step - loss: 0.3098 - val_loss: 0.7084\n",
      "Epoch 198/200\n",
      "1400/1400 [==============================] - 0s 157us/step - loss: 0.3127 - val_loss: 0.7062\n",
      "Epoch 199/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.2990 - val_loss: 0.7917\n",
      "Epoch 200/200\n",
      "1400/1400 [==============================] - 0s 159us/step - loss: 0.3042 - val_loss: 0.7880\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.pre_train_discriminator(d_epochs=200, d_pre_sig_train=sig_rd_train, d_pre_out_train=out_train, d_pre_sig_valid=sig_rd_valid, d_pre_out_valid=out_valid, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805\n"
     ]
    }
   ],
   "source": [
    "print(trainer.test_discriminator(sig_rd_test, out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.955\n"
     ]
    }
   ],
   "source": [
    "print(trainer.test_discriminator(sig_impersonate_ad, np.zeros((sig_impersonate_ad.shape[0],))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8531423"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainer.predict_discriminator(sig_auth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7397305"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainer.predict_discriminator(sig_unauth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16275488"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainer.predict_discriminator(sig_impersonate_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560, 2)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_impersonate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_im  = sig_impersonate.shape[0]\n",
    "\n",
    "n_test_im = int(test_frac*n_samples_im)\n",
    "n_valid_im = int(valid_frac*n_samples_im)\n",
    "n_train_im = n_samples_im - n_test_im - n_valid_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_train,sig_im_valid,sig_im_test=split(sig_impersonate,n_train_im,n_valid_im,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_train_re = sig_im_train.reshape((-1, 1, 2))\n",
    "sig_im_train_re_t = sig_im_train.reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_valid_re = sig_im_valid.reshape((-1, 1, 2))\n",
    "sig_im_valid_re_t = sig_im_valid.reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_test_re = sig_im_test.reshape((-1, 1,  2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GeneratorPretraining():\n",
    "#     def sampling(args):\n",
    "#         mean_mu, log_var = args\n",
    "#         epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n",
    "#         return mean_mu + K.exp(log_var/2)*epsilon\n",
    "#     def sampling_alt(args):\n",
    "#         mean_mu, log_var = args\n",
    "#         normal_dist = tf.distributions.Normal(mean_mu, tf.exp(log_var/2))\n",
    "#         return normal_dist.sample()\n",
    "    \n",
    "#     input=Input(shape=(None,2), dtype='float32', name='Input')\n",
    "#     out = LSTM(100, activation='relu', name='LSTM')(input)\n",
    "# #     denseMean = Dense(2, name='DenseMean')(out)\n",
    "# #     denseVariance = Dense(2, name='DenseVariance')(out)\n",
    "    \n",
    "#     #sampled_output = Lambda(sampling, name='sampling_layer')([denseMean, denseVariance])\n",
    "    \n",
    "#     #sampled_output = Lambda(sampling_alt, name='sampling_layer')([denseMean, denseVariance])\n",
    "    \n",
    "#     sampled_output = Dense(2, name='DummyDense')(out)\n",
    "    \n",
    "#     generator_pretraining = Model(input, sampled_output)\n",
    "#     return generator_pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator pre-training\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, 100)          41200       Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "DenseMean (Dense)               (None, 2)            202         LSTM[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "DenseVariance (Dense)           (None, 2)            202         LSTM[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sampling_layer (Lambda)         (None, 2)            0           DenseMean[0][0]                  \n",
      "                                                                 DenseVariance[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 41,604\n",
      "Trainable params: 41,604\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1792 samples, validate on 512 samples\n",
      "Epoch 1/100\n",
      "1792/1792 [==============================] - 2s 1ms/step - loss: 4.0692e-05 - val_loss: 1.2186e-06\n",
      "Epoch 2/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 8.3974e-07 - val_loss: 4.7074e-07\n",
      "Epoch 3/100\n",
      "1792/1792 [==============================] - 0s 153us/step - loss: 4.9200e-07 - val_loss: 7.1344e-07\n",
      "Epoch 4/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 5.6423e-07 - val_loss: 5.4070e-07\n",
      "Epoch 5/100\n",
      "1792/1792 [==============================] - 0s 150us/step - loss: 6.4145e-07 - val_loss: 5.7967e-07\n",
      "Epoch 6/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 7.6251e-07 - val_loss: 8.0421e-07\n",
      "Epoch 7/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 6.4863e-07 - val_loss: 9.0245e-07\n",
      "Epoch 8/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 6.7141e-07 - val_loss: 4.6319e-07\n",
      "Epoch 9/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 6.9207e-07 - val_loss: 1.0134e-06\n",
      "Epoch 10/100\n",
      "1792/1792 [==============================] - 0s 151us/step - loss: 6.9156e-07 - val_loss: 1.9116e-06\n",
      "Epoch 11/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 4.4396e-06 - val_loss: 4.7715e-06\n",
      "Epoch 12/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 1.4729e-06 - val_loss: 2.0884e-06\n",
      "Epoch 13/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 1.5193e-06 - val_loss: 3.0297e-06\n",
      "Epoch 14/100\n",
      "1792/1792 [==============================] - 0s 154us/step - loss: 1.8477e-06 - val_loss: 7.7771e-07\n",
      "Epoch 15/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 9.7967e-07 - val_loss: 1.1607e-06\n",
      "Epoch 16/100\n",
      "1792/1792 [==============================] - 0s 148us/step - loss: 9.6231e-07 - val_loss: 4.1106e-07\n",
      "Epoch 17/100\n",
      "1792/1792 [==============================] - 0s 153us/step - loss: 8.6219e-07 - val_loss: 1.1730e-06\n",
      "Epoch 18/100\n",
      "1792/1792 [==============================] - 0s 150us/step - loss: 2.2999e-06 - val_loss: 2.7370e-06\n",
      "Epoch 19/100\n",
      "1792/1792 [==============================] - 0s 173us/step - loss: 2.1721e-06 - val_loss: 1.3735e-06\n",
      "Epoch 20/100\n",
      "1792/1792 [==============================] - 0s 151us/step - loss: 1.9885e-06 - val_loss: 1.7116e-06\n",
      "Epoch 21/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 4.9567e-06 - val_loss: 7.8274e-06\n",
      "Epoch 22/100\n",
      "1792/1792 [==============================] - 0s 148us/step - loss: 4.5451e-06 - val_loss: 4.7408e-06\n",
      "Epoch 23/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 1.5555e-06 - val_loss: 4.2785e-07\n",
      "Epoch 24/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 9.4300e-07 - val_loss: 4.7790e-07\n",
      "Epoch 25/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 1.8679e-06 - val_loss: 1.6992e-06\n",
      "Epoch 26/100\n",
      "1792/1792 [==============================] - 0s 151us/step - loss: 1.0736e-06 - val_loss: 1.6217e-06\n",
      "Epoch 27/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 1.2821e-06 - val_loss: 1.3236e-06\n",
      "Epoch 28/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 1.9582e-06 - val_loss: 3.2427e-06\n",
      "Epoch 29/100\n",
      "1792/1792 [==============================] - 0s 151us/step - loss: 2.5322e-06 - val_loss: 2.6749e-06\n",
      "Epoch 30/100\n",
      "1792/1792 [==============================] - 0s 151us/step - loss: 1.4940e-06 - val_loss: 2.1653e-06\n",
      "Epoch 31/100\n",
      "1792/1792 [==============================] - 0s 152us/step - loss: 1.6321e-06 - val_loss: 9.5472e-07\n",
      "Epoch 32/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 1.5253e-06 - val_loss: 2.0322e-06\n",
      "Epoch 33/100\n",
      "1792/1792 [==============================] - 0s 152us/step - loss: 1.9095e-06 - val_loss: 3.1038e-06\n",
      "Epoch 34/100\n",
      "1792/1792 [==============================] - 0s 153us/step - loss: 3.3384e-06 - val_loss: 5.8835e-06\n",
      "Epoch 35/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 1.3616e-05 - val_loss: 4.1337e-06\n",
      "Epoch 36/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 4.9077e-06 - val_loss: 8.2391e-07\n",
      "Epoch 37/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.1754e-06 - val_loss: 1.3371e-06\n",
      "Epoch 38/100\n",
      "1792/1792 [==============================] - 0s 150us/step - loss: 5.4556e-07 - val_loss: 5.3926e-07\n",
      "Epoch 39/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 1.0592e-06 - val_loss: 9.9130e-07\n",
      "Epoch 40/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 9.3207e-07 - val_loss: 1.1696e-06\n",
      "Epoch 41/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 1.5256e-06 - val_loss: 6.1530e-07\n",
      "Epoch 42/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 1.2892e-06 - val_loss: 7.5356e-07\n",
      "Epoch 43/100\n",
      "1792/1792 [==============================] - 0s 150us/step - loss: 1.0936e-06 - val_loss: 9.5665e-07\n",
      "Epoch 44/100\n",
      "1792/1792 [==============================] - 0s 148us/step - loss: 1.4715e-06 - val_loss: 2.5441e-06\n",
      "Epoch 45/100\n",
      "1792/1792 [==============================] - 0s 148us/step - loss: 1.2169e-06 - val_loss: 3.1530e-06\n",
      "Epoch 46/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 2.9466e-06 - val_loss: 8.1749e-06\n",
      "Epoch 47/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 3.4455e-06 - val_loss: 1.5069e-06\n",
      "Epoch 48/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 3.7568e-06 - val_loss: 5.3725e-06\n",
      "Epoch 49/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 1.3155e-05 - val_loss: 5.8815e-06\n",
      "Epoch 50/100\n",
      "1792/1792 [==============================] - 0s 178us/step - loss: 2.4727e-06 - val_loss: 2.5988e-06\n",
      "Epoch 51/100\n",
      "1792/1792 [==============================] - 0s 156us/step - loss: 1.6863e-06 - val_loss: 1.1072e-06\n",
      "Epoch 52/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 1.2014e-06 - val_loss: 9.0930e-07\n",
      "Epoch 53/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 7.9117e-07 - val_loss: 8.9613e-07\n",
      "Epoch 54/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 1.1936e-06 - val_loss: 9.0438e-07\n",
      "Epoch 55/100\n",
      "1792/1792 [==============================] - 0s 150us/step - loss: 1.1542e-06 - val_loss: 1.2698e-06\n",
      "Epoch 56/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 8.4123e-07 - val_loss: 5.5094e-07\n",
      "Epoch 57/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.3740e-06 - val_loss: 8.4343e-07\n",
      "Epoch 58/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 9.7615e-07 - val_loss: 4.4102e-07\n",
      "Epoch 59/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.2795e-06 - val_loss: 8.0285e-06\n",
      "Epoch 60/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 4.7068e-06 - val_loss: 1.5121e-05\n",
      "Epoch 61/100\n",
      "1792/1792 [==============================] - 0s 161us/step - loss: 6.1966e-06 - val_loss: 3.8729e-06\n",
      "Epoch 62/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 1.5040e-06 - val_loss: 6.8379e-07\n",
      "Epoch 63/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 1.1750e-06 - val_loss: 1.0224e-06\n",
      "Epoch 64/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.6913e-06 - val_loss: 1.7770e-06\n",
      "Epoch 65/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 1.4409e-06 - val_loss: 6.1353e-07\n",
      "Epoch 66/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.4695e-06 - val_loss: 9.9638e-07\n",
      "Epoch 67/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 1.5190e-06 - val_loss: 7.3463e-07\n",
      "Epoch 68/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 2.2335e-06 - val_loss: 2.2621e-06\n",
      "Epoch 69/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.2783e-06 - val_loss: 1.1996e-06\n",
      "Epoch 70/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.4111e-06 - val_loss: 2.6424e-06\n",
      "Epoch 71/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 7.0315e-06 - val_loss: 1.0689e-05\n",
      "Epoch 72/100\n",
      "1792/1792 [==============================] - 0s 148us/step - loss: 6.1816e-06 - val_loss: 7.3220e-07\n",
      "Epoch 73/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 8.5380e-07 - val_loss: 6.1550e-07\n",
      "Epoch 74/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 5.8109e-07 - val_loss: 7.8250e-07\n",
      "Epoch 75/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 8.1298e-07 - val_loss: 1.1570e-06\n",
      "Epoch 76/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 7.5132e-07 - val_loss: 8.7855e-07\n",
      "Epoch 77/100\n",
      "1792/1792 [==============================] - 0s 153us/step - loss: 7.8110e-07 - val_loss: 1.5833e-06\n",
      "Epoch 78/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 1.1920e-06 - val_loss: 1.5220e-06\n",
      "Epoch 79/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 2.6894e-06 - val_loss: 2.3307e-06\n",
      "Epoch 80/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 2.6259e-06 - val_loss: 2.1911e-06\n",
      "Epoch 81/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 1.4803e-06 - val_loss: 1.3051e-06\n",
      "Epoch 82/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 2.6404e-06 - val_loss: 4.5404e-06\n",
      "Epoch 83/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.7979e-06 - val_loss: 2.8957e-06\n",
      "Epoch 84/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 6.3012e-06 - val_loss: 9.0293e-06\n",
      "Epoch 85/100\n",
      "1792/1792 [==============================] - 0s 150us/step - loss: 2.8076e-06 - val_loss: 9.0690e-07\n",
      "Epoch 86/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 8.0318e-07 - val_loss: 5.3196e-07\n",
      "Epoch 87/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 2.1538e-06 - val_loss: 4.0492e-06\n",
      "Epoch 88/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 2.5545e-06 - val_loss: 3.0564e-06\n",
      "Epoch 89/100\n",
      "1792/1792 [==============================] - 0s 151us/step - loss: 2.4112e-06 - val_loss: 3.4947e-06\n",
      "Epoch 90/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 6.4593e-06 - val_loss: 3.5774e-06\n",
      "Epoch 91/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 1.5414e-06 - val_loss: 1.7929e-06\n",
      "Epoch 92/100\n",
      "1792/1792 [==============================] - 0s 155us/step - loss: 7.1213e-07 - val_loss: 1.0705e-06\n",
      "Epoch 93/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 2.1037e-06 - val_loss: 4.9000e-06\n",
      "Epoch 94/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 9.0139e-06 - val_loss: 2.5594e-06\n",
      "Epoch 95/100\n",
      "1792/1792 [==============================] - 0s 156us/step - loss: 1.2714e-06 - val_loss: 2.9261e-06\n",
      "Epoch 96/100\n",
      "1792/1792 [==============================] - 0s 174us/step - loss: 2.2243e-06 - val_loss: 1.1679e-06\n",
      "Epoch 97/100\n",
      "1792/1792 [==============================] - 0s 158us/step - loss: 6.7764e-07 - val_loss: 8.2990e-07\n",
      "Epoch 98/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 1.1056e-06 - val_loss: 7.4952e-07\n",
      "Epoch 99/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 9.0318e-07 - val_loss: 6.9424e-07\n",
      "Epoch 100/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 8.4646e-07 - val_loss: 7.0974e-07\n"
     ]
    }
   ],
   "source": [
    "trainer.pre_train_generator(g_epochs=100, g_pre_data_train=[sig_im_train_re, sig_im_train_re_t], g_pre_data_valid=[sig_im_valid_re, sig_im_valid_re_t], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_generator = GeneratorPretraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_adam = keras.optimizers.Adam(0.0001)\n",
    "# dummy_generator.compile(g_adam, loss='mse')\n",
    "# print('Generator pre-training')\n",
    "# dummy_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_train_g_filepath = 't_pre_train_g_weights'\n",
    "# pre_train_g_c = [keras.callbacks.ModelCheckpoint(pre_train_g_filepath, monitor='val_loss',  save_best_only=True, save_weights_only=True)]\n",
    "# dummy_generator.fit(\n",
    "#     sig_im_train_re,\n",
    "#     sig_im_train_re,\n",
    "#     validation_data=(sig_im_valid_re, sig_im_valid_re),\n",
    "#     callbacks=pre_train_g_c,\n",
    "#     epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.256785 , 1.1796385]]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_im_train_re[1].reshape(1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25680447, 1.1814066 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict_pre_generator(sig_im_train_re[1].reshape(1,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0070243 ,  1.153871  ],\n",
       "       [-1.2718885 ,  0.09359182],\n",
       "       [-1.1191512 , -1.161263  ],\n",
       "       [-0.7922406 , -1.2390095 ],\n",
       "       [-0.9476444 , -0.93783224]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict_pre_generator(sig_im_test_re[:5].reshape(5,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.0047826,  1.1611896]]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_im_test_re[0].reshape(1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0048428,  1.1605326]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict_pre_generator(sig_im_test_re[0].reshape(1,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0048174  1.1605062]]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_pre_generator(sig_im_test_re[:1].reshape(1,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.00481268  1.16056222]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_generator(sig_im_test_re[:1].reshape(1,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.00486254  1.16054069]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_beta_generator(sig_im_test_re[:1].reshape(1,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2746066\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_curr_discriminator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test_curr_discriminator_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, pred=trainer.get_predicted_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60655093, -0.49042538], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig[252]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1338478, -2.6619375])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[252]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5380952 , 0.43678656], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.04717902, -0.65548495])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11324434]]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_discriminator(np.expand_dims(rf_system.transmit_real_symbol_block(orig, impersonator=True), 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_discriminator(np.expand_dims(rf_system.transmit_real_symbol_block(pred, impersonator=True), 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "symb=rf_system.get_real_pretx_symbol_block(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "symb_r=symb.reshape(1,256,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "st0=symb[0].reshape(1,1,2)\n",
    "st1=symb[1].reshape(1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19163832, -0.47349837], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symb[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "symb_r[:,-1,:] = st0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.60103184, -0.5632952 ]]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60103184, -0.5632952 ]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symb_r[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy_c=np.concatenate([symb_r, st1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.14769381,  0.06279915]]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14769381,  0.06279915]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy_c[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.reflect_pre_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Disc Accuracy: 0.069\n",
      "0, 1: Disc Accuracy: 0.334, Average reward: -0.456\n",
      "1, 1: Disc Accuracy: 0.699, Average reward: -0.122\n",
      "2, 1: Disc Accuracy: 0.440, Average reward: -0.151\n",
      "3, 1: Disc Accuracy: 0.393, Average reward: -0.283\n",
      "4, 1: Disc Accuracy: 0.259, Average reward: 0.425\n",
      "5, 1: Disc Accuracy: 0.775, Average reward: -0.362\n",
      "6, 1: Disc Accuracy: 0.058, Average reward: 0.082\n",
      "7, 1: Disc Accuracy: 0.069, Average reward: -0.389\n",
      "8, 1: Disc Accuracy: 0.103, Average reward: -0.454\n",
      "9, 1: Disc Accuracy: 0.808, Average reward: -0.438\n",
      "10, 1: Disc Accuracy: 0.214, Average reward: -0.374\n",
      "11, 1: Disc Accuracy: 0.056, Average reward: -0.329\n",
      "12, 1: Disc Accuracy: 0.002, Average reward: -0.459\n",
      "13, 1: Disc Accuracy: 0.935, Average reward: -0.500\n",
      "14, 1: Disc Accuracy: 0.051, Average reward: -0.437\n",
      "15, 1: Disc Accuracy: 0.110, Average reward: -0.486\n",
      "16, 1: Disc Accuracy: 0.111, Average reward: -0.497\n",
      "17, 1: Disc Accuracy: 0.923, Average reward: -0.486\n",
      "18, 1: Disc Accuracy: 0.094, Average reward: 0.443\n",
      "19, 1: Disc Accuracy: 0.046, Average reward: -0.114\n",
      "20, 1: Disc Accuracy: 0.788, Average reward: 0.115\n",
      "21, 1: Disc Accuracy: 0.678, Average reward: -0.489\n",
      "22, 1: Disc Accuracy: 0.003, Average reward: -0.135\n",
      "23, 1: Disc Accuracy: 0.014, Average reward: -0.491\n",
      "24, 1: Disc Accuracy: 0.546, Average reward: -0.206\n",
      "25, 1: Disc Accuracy: 0.555, Average reward: 0.498\n",
      "26, 1: Disc Accuracy: 0.998, Average reward: 0.481\n",
      "27, 1: Disc Accuracy: 0.997, Average reward: 0.404\n",
      "28, 1: Disc Accuracy: 0.997, Average reward: 0.500\n",
      "29, 1: Disc Accuracy: 0.991, Average reward: 0.493\n",
      "30, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "31, 1: Disc Accuracy: 0.999, Average reward: 0.493\n",
      "32, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "33, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "34, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "35, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "36, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "37, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "38, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "39, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "40, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "41, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "42, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "43, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "44, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "45, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "46, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "47, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "48, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "49, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "50, 1: Disc Accuracy: 1.000, Average reward: 0.499\n",
      "51, 1: Disc Accuracy: 1.000, Average reward: 0.499\n",
      "52, 1: Disc Accuracy: 0.368, Average reward: 0.179\n",
      "53, 1: Disc Accuracy: 0.449, Average reward: 0.497\n",
      "54, 1: Disc Accuracy: 0.037, Average reward: -0.453\n",
      "55, 1: Disc Accuracy: 0.037, Average reward: 0.411\n",
      "56, 1: Disc Accuracy: 0.999, Average reward: 0.397\n",
      "57, 1: Disc Accuracy: 0.967, Average reward: -0.435\n",
      "58, 1: Disc Accuracy: 0.411, Average reward: -0.498\n",
      "59, 1: Disc Accuracy: 0.898, Average reward: 0.222\n",
      "60, 1: Disc Accuracy: 0.383, Average reward: -0.474\n",
      "61, 1: Disc Accuracy: 0.522, Average reward: -0.312\n",
      "62, 1: Disc Accuracy: 0.019, Average reward: -0.487\n",
      "63, 1: Disc Accuracy: 0.005, Average reward: -0.479\n",
      "64, 1: Disc Accuracy: 0.005, Average reward: -0.427\n",
      "65, 1: Disc Accuracy: 0.683, Average reward: -0.472\n",
      "66, 1: Disc Accuracy: 0.014, Average reward: -0.471\n",
      "67, 1: Disc Accuracy: 0.000, Average reward: -0.496\n",
      "68, 1: Disc Accuracy: 0.730, Average reward: -0.494\n",
      "69, 1: Disc Accuracy: 0.008, Average reward: -0.495\n",
      "70, 1: Disc Accuracy: 0.087, Average reward: 0.447\n",
      "71, 1: Disc Accuracy: 0.984, Average reward: 0.477\n",
      "72, 1: Disc Accuracy: 0.995, Average reward: 0.491\n",
      "73, 1: Disc Accuracy: 1.000, Average reward: 0.139\n",
      "74, 1: Disc Accuracy: 0.998, Average reward: 0.224\n",
      "75, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "76, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "77, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "78, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "79, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "80, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "81, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "82, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "83, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "84, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "85, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "86, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "87, 1: Disc Accuracy: 1.000, Average reward: 0.500\n",
      "88, 1: Disc Accuracy: 1.000, Average reward: 0.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-22a2d312a5fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\My Drive\\Research\\transmitter_impersonation_alt\\SigGAN\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, steps, g_steps, g_weights_path, verbose)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[1;31m#print('{:d}, {:d}: Average reward: {:.3f}'.format(step, 1, np.mean(rewards)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:d}, {:d}: Disc Accuracy: {:.3f}, Average reward: {:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_curr_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[1;31m#if(~verbose): print('Disc Accuracy: {:.3f}'.format(self.predict_curr_discriminator(self.rf_system)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\My Drive\\Research\\transmitter_impersonation_alt\\SigGAN\\train.py\u001b[0m in \u001b[0;36mpredict_curr_discriminator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_curr_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mperturbed_signal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrf_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransmit_real_symbol_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_sequence_alt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrf_system\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_real_pretx_symbol_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimpersonator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperturbed_signal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\My Drive\\Research\\transmitter_impersonation_alt\\SigGAN\\models.py\u001b[0m in \u001b[0;36msampling_sequence_alt\u001b[1;34m(self, T, orig_symbols)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_variance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_symbols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[1;31m#print(mean, log_variance)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_variance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\My Drive\\Research\\transmitter_impersonation_alt\\SigGAN\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, state, stateful)\u001b[0m\n\u001b[0;32m     82\u001b[0m             self.state_in : state}\n\u001b[0;32m     83\u001b[0m         mean, log_variance = self.sess.run(\n\u001b[1;32m---> 84\u001b[1;33m             [self.mean, self.log_variance], feed_dict)\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_variance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
