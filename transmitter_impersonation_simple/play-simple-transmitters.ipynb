{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WirelessSystem.system import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from SigGAN.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(sig_u):\n",
    "    pwr = np.sqrt(np.mean(np.sum(sig_u**2,axis = -1),axis = -1))\n",
    "    sig_u = sig_u/pwr[:,None,None]\n",
    "    print(sig_u.shape)\n",
    "    return sig_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(vec1,vec2,seed = 0):\n",
    "    np.random.seed(0)\n",
    "    shfl_indx = np.arange(vec1.shape[0])\n",
    "    np.random.shuffle(shfl_indx)\n",
    "    shfl_indx = shfl_indx.astype('int')\n",
    "    vec1 = vec1[shfl_indx]\n",
    "    vec2 = np.copy(vec2[shfl_indx])\n",
    "    return vec1,vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(vec,n_train,n_valid,n_test):\n",
    "    vec_train = vec[0:n_train]\n",
    "    vec_valid = vec[n_train:n_train+n_valid]\n",
    "    vec_test = vec[n_train+n_valid:]\n",
    "    return vec_train,vec_valid,vec_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols = 256 // 2\n",
    "n_authorized = 10\n",
    "n_unauthorized = 0\n",
    "sess_name = 'test_transmitters_snr'\n",
    "snr=20\n",
    "suffix='_t_%d_snr_%d'%(n_authorized,snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_t_10_snr_20'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_system = RFSystem(n_authorized = n_authorized, n_unauthorized = n_unauthorized, snr = snr, sess_name = sess_name, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_system = RFSystem(sess_dir=sess_name, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_auth_, txid_auth = rf_system.get_n_received_symbol_blocks(5000, n_symbols, authorized = 0)\n",
    "sig_unauth_, txid_unauth = rf_system.get_n_received_symbol_blocks(5000, n_symbols, authorized = 1)\n",
    "sig_impersonate_, _ = rf_system.get_n_received_symbol_blocks(10, n_symbols, authorized = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_auth=sig_auth_\n",
    "sig_unauth=sig_unauth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_impersonate_ad_, _ = rf_system.get_n_received_symbol_blocks(5000, n_symbols, authorized = 2)\n",
    "sig_impersonate_ad=sig_impersonate_ad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_impersonate=sig_impersonate_.reshape((-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "B = 64 # batch size\n",
    "T = 256 # Max length of sentence\n",
    "g_H = 100 # Generator LSTM hidden size\n",
    "g_lr = 1e-3\n",
    "d_dropout = 0.5 # dropout ratio\n",
    "d_lr = 1e-3\n",
    "\n",
    "n_sample=1 # Number of Monte Calro Search\n",
    "generate_samples = 2000 # Number of generated sentences\n",
    "\n",
    "# Pretraining parameters\n",
    "g_pre_lr = 1e-3\n",
    "d_pre_lr = 1e-3\n",
    "g_pre_epochs= 60\n",
    "d_pre_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\My Drive\\Research\\transmitter_impersonation_simple\\SigGAN\\models.py:104: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\My Drive\\Research\\transmitter_impersonation_simple\\SigGAN\\models.py:106: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From G:\\My Drive\\Research\\transmitter_impersonation_simple\\SigGAN\\models.py:124: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(rf_system, B, T, n_authorized, g_H, d_dropout, g_lr=g_lr, d_lr=d_lr, n_sample=n_sample, generate_samples=generate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rd = np.concatenate([sig_auth,sig_impersonate_ad])\n",
    "txid_rd = np.concatenate([txid_auth,np.ones((sig_impersonate_ad.shape[0],))*n_authorized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig_rd = np.concatenate([sig_auth,sig_unauth])\n",
    "# txid_rd = np.concatenate([txid_auth,np.ones((sig_unauth.shape[0],))*n_authorized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rd, txid_rd = shuffle(sig_rd, txid_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txid_disc = txid_rd == n_authorized\n",
    "txid_disc = np.invert(txid_disc)\n",
    "txid_disc = txid_disc.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(txid_disc))\n",
    "print(np.unique(txid_rd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.1\n",
    "valid_frac  = 0.2\n",
    "\n",
    "n_samples  = sig_rd.shape[0]\n",
    "\n",
    "n_test = int(test_frac*n_samples)\n",
    "n_valid = int(valid_frac*n_samples)\n",
    "n_train = n_samples - n_test - n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_rd_train,sig_rd_valid,sig_rd_test=split(sig_rd,n_train,n_valid,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train,out_valid,out_test=split(txid_disc,n_train,n_valid,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_pre_train_d(rf_system.full_sess_dir, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.pre_train_discriminator(d_epochs=100, d_pre_sig_train=sig_rd_train, d_pre_out_train=out_train, d_pre_sig_valid=sig_rd_valid, d_pre_out_valid=out_valid, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_pre_train_d(rf_system.full_sess_dir, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9155714285714286\n"
     ]
    }
   ],
   "source": [
    "print(trainer.test_discriminator(sig_rd_train, out_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9095\n"
     ]
    }
   ],
   "source": [
    "print(trainer.test_discriminator(sig_rd_valid, out_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926\n"
     ]
    }
   ],
   "source": [
    "print(trainer.test_discriminator(sig_rd_test, out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9948\n"
     ]
    }
   ],
   "source": [
    "print(trainer.test_discriminator(sig_impersonate_ad, np.zeros((sig_impersonate_ad.shape[0],))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85650325"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainer.predict_discriminator(sig_auth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14890347"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainer.predict_discriminator(sig_impersonate_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_im  = sig_impersonate.shape[0]\n",
    "\n",
    "n_test_im = int(test_frac*n_samples_im)\n",
    "n_valid_im = int(valid_frac*n_samples_im)\n",
    "n_train_im = n_samples_im - n_test_im - n_valid_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_train,sig_im_valid,sig_im_test=split(sig_impersonate,n_train_im,n_valid_im,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_train_re = sig_im_train.reshape((-1, 1, 2))\n",
    "sig_im_train_re_t = sig_im_train.reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_valid_re = sig_im_valid.reshape((-1, 1, 2))\n",
    "sig_im_valid_re_t = sig_im_valid.reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_im_test_re = sig_im_test.reshape((-1, 1,  2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Generator pre-training\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, 100)          41200       Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "DenseMean (Dense)               (None, 2)            202         LSTM[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "DenseVariance (Dense)           (None, 2)            202         LSTM[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sampling_layer (Lambda)         (None, 2)            0           DenseMean[0][0]                  \n",
      "                                                                 DenseVariance[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 41,604\n",
      "Trainable params: 41,604\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\samur\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1792 samples, validate on 512 samples\n",
      "Epoch 1/100\n",
      "1792/1792 [==============================] - 1s 622us/step - loss: 1.8969 - val_loss: 1.6683\n",
      "Epoch 2/100\n",
      "1792/1792 [==============================] - 0s 147us/step - loss: 1.2729 - val_loss: 0.8355\n",
      "Epoch 3/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 0.5110 - val_loss: 0.3088\n",
      "Epoch 4/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 0.2329 - val_loss: 0.1746\n",
      "Epoch 5/100\n",
      "1792/1792 [==============================] - 0s 137us/step - loss: 0.1419 - val_loss: 0.1111\n",
      "Epoch 6/100\n",
      "1792/1792 [==============================] - 0s 130us/step - loss: 0.0952 - val_loss: 0.0796\n",
      "Epoch 7/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 0.0647 - val_loss: 0.0539\n",
      "Epoch 8/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 0.0456 - val_loss: 0.0403\n",
      "Epoch 9/100\n",
      "1792/1792 [==============================] - 0s 135us/step - loss: 0.0355 - val_loss: 0.0307\n",
      "Epoch 10/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 0.0254 - val_loss: 0.0219\n",
      "Epoch 11/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 0.0205 - val_loss: 0.0167\n",
      "Epoch 12/100\n",
      "1792/1792 [==============================] - 0s 135us/step - loss: 0.0162 - val_loss: 0.0148\n",
      "Epoch 13/100\n",
      "1792/1792 [==============================] - 0s 152us/step - loss: 0.0131 - val_loss: 0.0099\n",
      "Epoch 14/100\n",
      "1792/1792 [==============================] - 0s 150us/step - loss: 0.0104 - val_loss: 0.0095\n",
      "Epoch 15/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 0.0087 - val_loss: 0.0076\n",
      "Epoch 16/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 0.0076 - val_loss: 0.0067\n",
      "Epoch 17/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 18/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 0.0051 - val_loss: 0.0043\n",
      "Epoch 19/100\n",
      "1792/1792 [==============================] - 0s 132us/step - loss: 0.0049 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 21/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 0.0039 - val_loss: 0.0029\n",
      "Epoch 22/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 23/100\n",
      "1792/1792 [==============================] - ETA: 0s - loss: 0.002 - 0s 136us/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 24/100\n",
      "1792/1792 [==============================] - 0s 135us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 25/100\n",
      "1792/1792 [==============================] - 0s 133us/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 26/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 27/100\n",
      "1792/1792 [==============================] - 0s 129us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 28/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "1792/1792 [==============================] - 0s 131us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 30/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 31/100\n",
      "1792/1792 [==============================] - 0s 151us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 33/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 34/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 35/100\n",
      "1792/1792 [==============================] - 0s 135us/step - loss: 9.5879e-04 - val_loss: 8.9221e-04\n",
      "Epoch 36/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 8.7150e-04 - val_loss: 8.1722e-04\n",
      "Epoch 37/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 8.3335e-04 - val_loss: 7.9805e-04\n",
      "Epoch 38/100\n",
      "1792/1792 [==============================] - 0s 137us/step - loss: 7.4523e-04 - val_loss: 7.6672e-04\n",
      "Epoch 39/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 7.2908e-04 - val_loss: 6.3579e-04\n",
      "Epoch 40/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 7.1293e-04 - val_loss: 6.6106e-04\n",
      "Epoch 41/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 6.3781e-04 - val_loss: 5.6136e-04\n",
      "Epoch 42/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 5.9260e-04 - val_loss: 5.6385e-04\n",
      "Epoch 43/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 5.8222e-04 - val_loss: 5.1861e-04\n",
      "Epoch 44/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 5.0278e-04 - val_loss: 5.1727e-04\n",
      "Epoch 45/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 4.8159e-04 - val_loss: 4.6657e-04\n",
      "Epoch 46/100\n",
      "1792/1792 [==============================] - 0s 133us/step - loss: 4.4750e-04 - val_loss: 4.1087e-04\n",
      "Epoch 47/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 4.1640e-04 - val_loss: 3.7756e-04\n",
      "Epoch 48/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 4.4082e-04 - val_loss: 3.9212e-04\n",
      "Epoch 49/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 3.7701e-04 - val_loss: 3.7664e-04\n",
      "Epoch 50/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 3.5993e-04 - val_loss: 3.4086e-04\n",
      "Epoch 51/100\n",
      "1792/1792 [==============================] - 0s 129us/step - loss: 3.4362e-04 - val_loss: 3.2400e-04\n",
      "Epoch 52/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 3.4972e-04 - val_loss: 2.7320e-04\n",
      "Epoch 53/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 3.1469e-04 - val_loss: 2.5753e-04\n",
      "Epoch 54/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 3.1059e-04 - val_loss: 3.1103e-04\n",
      "Epoch 55/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 2.8456e-04 - val_loss: 2.6308e-04\n",
      "Epoch 56/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 2.6399e-04 - val_loss: 2.5359e-04\n",
      "Epoch 57/100\n",
      "1792/1792 [==============================] - 0s 144us/step - loss: 2.4677e-04 - val_loss: 2.4343e-04\n",
      "Epoch 58/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 2.2634e-04 - val_loss: 2.3123e-04\n",
      "Epoch 59/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 2.2867e-04 - val_loss: 2.0962e-04\n",
      "Epoch 60/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 2.2899e-04 - val_loss: 2.0987e-04\n",
      "Epoch 61/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 2.1535e-04 - val_loss: 1.9352e-04\n",
      "Epoch 62/100\n",
      "1792/1792 [==============================] - 0s 135us/step - loss: 2.0005e-04 - val_loss: 1.9245e-04\n",
      "Epoch 63/100\n",
      "1792/1792 [==============================] - 0s 136us/step - loss: 1.9115e-04 - val_loss: 1.8059e-04\n",
      "Epoch 64/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 1.7207e-04 - val_loss: 1.7085e-04\n",
      "Epoch 65/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 1.7536e-04 - val_loss: 1.6255e-04\n",
      "Epoch 66/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 1.7715e-04 - val_loss: 1.4493e-04\n",
      "Epoch 67/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 1.6794e-04 - val_loss: 1.4098e-04\n",
      "Epoch 68/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 1.5497e-04 - val_loss: 1.4075e-04\n",
      "Epoch 69/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 1.4288e-04 - val_loss: 1.3787e-04\n",
      "Epoch 70/100\n",
      "1792/1792 [==============================] - 0s 143us/step - loss: 1.5055e-04 - val_loss: 1.5874e-04\n",
      "Epoch 71/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 1.2999e-04 - val_loss: 1.2630e-04\n",
      "Epoch 72/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 1.3729e-04 - val_loss: 1.2572e-04\n",
      "Epoch 73/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 1.2759e-04 - val_loss: 1.1296e-04\n",
      "Epoch 74/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 1.2198e-04 - val_loss: 1.0952e-04\n",
      "Epoch 75/100\n",
      "1792/1792 [==============================] - 0s 146us/step - loss: 1.2896e-04 - val_loss: 1.2102e-04\n",
      "Epoch 76/100\n",
      "1792/1792 [==============================] - 0s 134us/step - loss: 1.1252e-04 - val_loss: 1.1554e-04\n",
      "Epoch 77/100\n",
      "1792/1792 [==============================] - 0s 137us/step - loss: 1.1506e-04 - val_loss: 1.1473e-04\n",
      "Epoch 78/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 1.0064e-04 - val_loss: 9.8891e-05\n",
      "Epoch 79/100\n",
      "1792/1792 [==============================] - 0s 132us/step - loss: 1.0059e-04 - val_loss: 8.1293e-05\n",
      "Epoch 80/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 1.0142e-04 - val_loss: 9.0823e-05\n",
      "Epoch 81/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 8.9506e-05 - val_loss: 8.1163e-05\n",
      "Epoch 82/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 9.2919e-05 - val_loss: 7.8592e-05\n",
      "Epoch 83/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 9.0030e-05 - val_loss: 7.5820e-05\n",
      "Epoch 84/100\n",
      "1792/1792 [==============================] - 0s 145us/step - loss: 8.4678e-05 - val_loss: 7.0971e-05\n",
      "Epoch 85/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 7.7640e-05 - val_loss: 7.3154e-05\n",
      "Epoch 86/100\n",
      "1792/1792 [==============================] - 0s 149us/step - loss: 7.4229e-05 - val_loss: 8.0380e-05\n",
      "Epoch 87/100\n",
      "1792/1792 [==============================] - 0s 138us/step - loss: 6.9429e-05 - val_loss: 7.6337e-05\n",
      "Epoch 88/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 7.3235e-05 - val_loss: 6.0490e-05\n",
      "Epoch 89/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 6.7941e-05 - val_loss: 6.1961e-05\n",
      "Epoch 90/100\n",
      "1792/1792 [==============================] - 0s 135us/step - loss: 6.7990e-05 - val_loss: 5.5613e-05\n",
      "Epoch 91/100\n",
      "1792/1792 [==============================] - 0s 148us/step - loss: 6.6114e-05 - val_loss: 5.6104e-05\n",
      "Epoch 92/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 6.1383e-05 - val_loss: 5.1173e-05\n",
      "Epoch 93/100\n",
      "1792/1792 [==============================] - 0s 141us/step - loss: 5.8889e-05 - val_loss: 5.7682e-05\n",
      "Epoch 94/100\n",
      "1792/1792 [==============================] - 0s 139us/step - loss: 5.8020e-05 - val_loss: 6.0046e-05\n",
      "Epoch 95/100\n",
      "1792/1792 [==============================] - 0s 137us/step - loss: 5.5967e-05 - val_loss: 5.2953e-05\n",
      "Epoch 96/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 5.2832e-05 - val_loss: 4.7433e-05\n",
      "Epoch 97/100\n",
      "1792/1792 [==============================] - 0s 131us/step - loss: 5.3359e-05 - val_loss: 4.2513e-05\n",
      "Epoch 98/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 4.7875e-05 - val_loss: 4.5462e-05\n",
      "Epoch 99/100\n",
      "1792/1792 [==============================] - 0s 142us/step - loss: 4.8124e-05 - val_loss: 3.9397e-05\n",
      "Epoch 100/100\n",
      "1792/1792 [==============================] - 0s 140us/step - loss: 4.3779e-05 - val_loss: 3.8990e-05\n"
     ]
    }
   ],
   "source": [
    "trainer.pre_train_generator(g_epochs=100, g_pre_data_train=[sig_im_train_re, sig_im_train_re_t], g_pre_data_valid=[sig_im_valid_re, sig_im_valid_re_t], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0817413, -0.996754 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_im_test_re[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0778791 -0.9938676]]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_pre_generator(sig_im_test_re[:1].reshape(1,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0763426  -0.99353707]]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_generator(sig_im_test_re[:1].reshape(1,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0815829  -0.99369425]]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_beta_generator(sig_im_test_re[:1].reshape(1,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14950863\n"
     ]
    }
   ],
   "source": [
    "print(trainer.predict_curr_discriminator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.reset_generator(lr=1e-3, epsilon=0.08, beta=15000, binary_reward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.reflect_pre_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Disc Accuracy: 0.105\n",
      "0: Disc Batch Accuracy: 0.000\n",
      "0, 1: Disc Accuracy: 0.255, Average reward: 0.000\n",
      "1, 1: Disc Accuracy: 0.175, Average reward: 0.000\n",
      "2, 1: Disc Accuracy: 0.499, Average reward: 0.000\n",
      "3, 1: Disc Accuracy: 0.862, Average reward: 0.000\n",
      "4, 1: Disc Accuracy: 0.828, Average reward: 1.000\n",
      "5, 1: Disc Accuracy: 0.970, Average reward: 1.000\n",
      "6, 1: Disc Accuracy: 0.972, Average reward: 1.000\n",
      "7, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "8, 1: Disc Accuracy: 0.995, Average reward: 1.000\n",
      "9, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "10, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "11, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "12, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "13, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "14, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "15, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "16, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "17, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "18, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "19, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "19: Disc Batch Accuracy: 1.000\n",
      "20, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "21, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "22, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "23, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "24, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "25, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "26, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "27, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "28, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "29, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "30, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "31, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "32, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "33, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "34, 1: Disc Accuracy: 0.994, Average reward: 1.000\n",
      "35, 1: Disc Accuracy: 0.991, Average reward: 1.000\n",
      "36, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "37, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "38, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "39, 1: Disc Accuracy: 0.995, Average reward: 1.000\n",
      "39: Disc Batch Accuracy: 1.000\n",
      "40, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "41, 1: Disc Accuracy: 0.995, Average reward: 1.000\n",
      "42, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "43, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "44, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "45, 1: Disc Accuracy: 0.992, Average reward: 1.000\n",
      "46, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "47, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "48, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "49, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "50, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "51, 1: Disc Accuracy: 0.988, Average reward: 1.000\n",
      "52, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "53, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "54, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "55, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "56, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "57, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "58, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "59, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "59: Disc Batch Accuracy: 1.000\n",
      "60, 1: Disc Accuracy: 0.995, Average reward: 1.000\n",
      "61, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "62, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "63, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "64, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "65, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "66, 1: Disc Accuracy: 0.993, Average reward: 1.000\n",
      "67, 1: Disc Accuracy: 0.994, Average reward: 1.000\n",
      "68, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "69, 1: Disc Accuracy: 0.994, Average reward: 1.000\n",
      "70, 1: Disc Accuracy: 0.990, Average reward: 1.000\n",
      "71, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "72, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "73, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "74, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "75, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "76, 1: Disc Accuracy: 0.975, Average reward: 1.000\n",
      "77, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "78, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "79, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "79: Disc Batch Accuracy: 1.000\n",
      "80, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "81, 1: Disc Accuracy: 0.994, Average reward: 1.000\n",
      "82, 1: Disc Accuracy: 0.993, Average reward: 1.000\n",
      "83, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "84, 1: Disc Accuracy: 0.985, Average reward: 1.000\n",
      "85, 1: Disc Accuracy: 0.984, Average reward: 1.000\n",
      "86, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "87, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "88, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "89, 1: Disc Accuracy: 0.988, Average reward: 1.000\n",
      "90, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "91, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "92, 1: Disc Accuracy: 0.944, Average reward: 1.000\n",
      "93, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "94, 1: Disc Accuracy: 0.914, Average reward: 1.000\n",
      "95, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "96, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "97, 1: Disc Accuracy: 0.992, Average reward: 1.000\n",
      "98, 1: Disc Accuracy: 0.994, Average reward: 1.000\n",
      "99, 1: Disc Accuracy: 0.942, Average reward: 1.000\n",
      "99: Disc Batch Accuracy: 1.000\n",
      "100, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "101, 1: Disc Accuracy: 0.967, Average reward: 1.000\n",
      "102, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "103, 1: Disc Accuracy: 0.986, Average reward: 1.000\n",
      "104, 1: Disc Accuracy: 0.982, Average reward: 1.000\n",
      "105, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "106, 1: Disc Accuracy: 0.937, Average reward: 1.000\n",
      "107, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "108, 1: Disc Accuracy: 0.992, Average reward: 1.000\n",
      "109, 1: Disc Accuracy: 0.981, Average reward: 1.000\n",
      "110, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "111, 1: Disc Accuracy: 0.942, Average reward: 1.000\n",
      "112, 1: Disc Accuracy: 0.843, Average reward: 1.000\n",
      "113, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "114, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "115, 1: Disc Accuracy: 0.909, Average reward: 1.000\n",
      "116, 1: Disc Accuracy: 0.995, Average reward: 1.000\n",
      "117, 1: Disc Accuracy: 0.982, Average reward: 1.000\n",
      "118, 1: Disc Accuracy: 0.984, Average reward: 1.000\n",
      "119, 1: Disc Accuracy: 0.990, Average reward: 1.000\n",
      "119: Disc Batch Accuracy: 1.000\n",
      "120, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "121, 1: Disc Accuracy: 0.968, Average reward: 1.000\n",
      "122, 1: Disc Accuracy: 0.994, Average reward: 1.000\n",
      "123, 1: Disc Accuracy: 0.964, Average reward: 1.000\n",
      "124, 1: Disc Accuracy: 0.991, Average reward: 1.000\n",
      "125, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "126, 1: Disc Accuracy: 0.993, Average reward: 1.000\n",
      "127, 1: Disc Accuracy: 0.994, Average reward: 1.000\n",
      "128, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "129, 1: Disc Accuracy: 0.996, Average reward: 1.000\n",
      "130, 1: Disc Accuracy: 0.998, Average reward: 1.000\n",
      "131, 1: Disc Accuracy: 0.980, Average reward: 1.000\n",
      "132, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "133, 1: Disc Accuracy: 0.988, Average reward: 1.000\n",
      "134, 1: Disc Accuracy: 0.829, Average reward: 1.000\n",
      "135, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "136, 1: Disc Accuracy: 0.942, Average reward: 1.000\n",
      "137, 1: Disc Accuracy: 0.962, Average reward: 1.000\n",
      "138, 1: Disc Accuracy: 0.972, Average reward: 1.000\n",
      "139, 1: Disc Accuracy: 0.973, Average reward: 1.000\n",
      "139: Disc Batch Accuracy: 1.000\n",
      "140, 1: Disc Accuracy: 0.993, Average reward: 1.000\n",
      "141, 1: Disc Accuracy: 1.000, Average reward: 1.000\n",
      "142, 1: Disc Accuracy: 0.999, Average reward: 1.000\n",
      "143, 1: Disc Accuracy: 0.991, Average reward: 1.000\n",
      "144, 1: Disc Accuracy: 0.997, Average reward: 1.000\n",
      "145, 1: Disc Accuracy: 0.940, Average reward: 1.000\n",
      "146, 1: Disc Accuracy: 0.977, Average reward: 1.000\n"
     ]
    }
   ],
   "source": [
    "trainer.train(steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
